# smallest clean way to add W&B logging to my training scrips:


1) import wandb:

        import wandb

2) Initialize a run **ONCE** (inside main() or before training loop):

       run_wandb = wandb.init(
          project="<NAME OF YOUR PROJECT>",
          entity="ioannis_dilmperis-vrije-universiteit-brussel",
          config=vars(args),   # logs all argparse args
       )

     
3) Log metrics each epoch (inside the loop, after computing loss)


        wandb.log(
            {
                "loss": loss.item(),
                "distortion": loss_dist_weighted.item(),
                "cross_entropy": loss_cross_entropy.item(),
                "eval_performance": accuracy.item(),
                "tau": float(tau),
                "lr": optimizer.param_groups[0]["lr"],
            },
            step=epoch,
        )

5) Finish once, at the end, after the whole train & evaluation:

         run_wandb.finish()

---

## Full Example:

                import wandb
                
                wandb.init(project="my-project")  # (optional) add entity=...
                
                for epoch in range(epochs):
                    # ... forward, compute loss, backward, step ...
                
                    wandb.log({
                        "loss": loss.item(),
                        "distortion": loss_dist_weighted.item(),
                        "cross_entropy": loss_cross_entropy.item(),
                        "eval_performance": accuracy.item(),
                        "tau": float(tau),
                    }, step=epoch)
                
                wandb.finish()
